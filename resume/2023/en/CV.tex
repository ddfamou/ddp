% LaTeX file for resume
% This file uses the resume document class (res.cls)

\documentclass{res}
%\usepackage{helvetica} % uses helvetica postscript font (download helvetica.sty)
%\usepackage{newcent}   % uses new century schoolbook postscript font
\newsectionwidth{0pt}  % So the text is not indented under section headings
\usepackage{fancyhdr}  % use this package to get a 2 line header
\renewcommand{\headrulewidth}{0pt} % suppress line drawn by default by fancyhdr
\setlength{\headheight}{0pt} % allow room for 2-line header
\setlength{\headsep}{0pt}  % space between header and text
\setlength{\headheight}{0pt} % allow room for 2-line header
\usepackage[a4paper,vmargin={10mm,0mm},hmargin={15mm,15mm}]{geometry}
\pagestyle{fancy}     % set pagestyle for document
\rhead{ {\it M. SHAN}\\{\it p. \thepage} } % put text in header (right side)
\cfoot{}                                     % the foot is empty
\topmargin=-0.5in % start text higher on the page

\begin{document}
\thispagestyle{empty} % this page has no header
\name{\Large Mengfan (Fox) \underline{SHAN}\\[12pt]}% the \\[12pt] adds a blank line after name

\address{{\bf Address:} 466 Tianbao Road, Shanghai, 200086 \hspace{0.25in}  {\bf Tel:} 18017661124 \hspace{0.25in}  {\bf   Email:} dd.famous@gmail.com}

\begin{resume}
\section{{WORKING EXPERIENCE}}
\vspace{-10pt}
\hrulefill\\

{\bf Shuhe Group}, Shanghai \hfill Since April, 2019 \\
I am now working in Shanghai Shuhe Group, in charge of Platform Team, BigData Department. 
My work is mainly related to BigData middle-wares, such as Hadoop, Hive, HBase, Spark, etc. I design, deploy, monitor and optimize all BigData services, by cooperating with Data Warehouse Teams and Data Application Teams to provide high-available data services for both customers and data scientists inside company. I also take responsibility of problem solving, new component introducing, etc, to improve performance and stability of BigData platforms.

{\bf Shuhe Group}, Shanghai \hfill Since April, 2019 \\
I am now working in Shanghai Shuhe Group, in charge of Platform Team, BigData Department. 
My work is mainly related to BigData middle-wares, such as Hadoop, Hive, HBase, Spark, etc. I design, deploy, monitor and optimize all BigData services, by cooperating with Data Warehouse Teams and Data Application Teams to provide high-available data services for both customers and data scientists inside company. I also take responsibility of problem solving, new component introducing, etc, to improve performance and stability of BigData platforms.

\vspace{-5pt}
{\bf Works Applications}, Shanghai \hfill August, 2016 ~ April, 2019\\
I joined Team EDP2 (Base Technologies Div) in 12/2016 and became the TL in 12/2017. 
EDP2 is a developing and operating platform for enterprise application on cloud, for higher speed, better stability and cost efficiency.
EDP2 includes a server for job management and a Hadoop/Spark cluster for job execution. 

\section{{PROJECTS}}
\vspace{-10pt}
\hrulefill\\
{\bf FeatureHub} \\
FeatureHub is a platform managing and providing features, for training models and online-prediction. Unlike traditional feature platforms, which focus on data processing, FeatureHub provides processed data for both online and offline services with the unified calibre. It provides real time features for online services with low latency, as well as historical data for training.

\vspace{-5pt}
{\bf Scalable} \\
In original design, the data warehouse is on one single Hadoop based cluster. 
The cluster is slow to scale up and hard to scale down and it is also a SPoF for most services.
HDFS is replaced by object storages and data processing jobs are separated to many state-less auto-scale EMR clusters for low cost and better availability. 

\vspace{-5pt}
{\bf Customization}\\
Open source softwares could hardly cover all requirements.
We did a number of customizations on open source softwares, for column based sensitive data check, SQL filter, data relationship. We also make AdHoc queries can join data from different sources, such as Hive, HBase, Mongo, Kudu, etc, and sometimes patch bugs such as HDFS-9406, HIVE-19994. 

\vspace{-5pt}
{\bf SLA}\\
Ensure stability and performance of BigData services. Services provided to customers should have an SLA of 99.99\% within 200ms.
For example, we had a HBase cluster was unstable. The average response time is over 100ms and MTTR always took hours. After re-designing, the cluster never crush again, the average response time is on 20ms and the performance is extremely improved. 

\vspace{-5pt}
{\bf Multi-Clouds} \\ 
The BigData cluster was migrated from Cloudera to AWS EMR, then from AWS to AliCloud. 
To minimize migration impact and downtime to business, the platform foundation needs to be compatible with changes and we need to provides all kinds of automation tools for migration and further management. 

\section{SKILLS}
\vspace{-10pt}
\hrulefill\\
{\sl Languages:} Java, python, Ansible, shell\\
{\sl Middleware:} Hive, HBase, Hadoop, Spark, Flume, Flink, Zookeeper, Kafka, neo4j, Presto, HugeGraph\\
{\sl Skills:} AWS, Aliyun

\section{{EDUCATIONS}}
\vspace{-10pt}
\hrulefill\\
{\bf Nanjing University}, Nanjing \hfill  09/2010 - 06/2016 \\
{\sl Ph.D. in Computer Software and Theory}, Department of Computer Science and Technology\\
{\bf University of Minnesota, Twin Cities}, MN \hfill 10/2014 - 10/2015 \\
{\sl Visiting Scholar supervised by Prof. Tian He}, Department of Computer Science and Engineering\\
{\bf Nanjing University}, Nanjing  \hfill 09/2006 - 06/2010 \\
{\sl Bachelor}, Department of  Computer Science and Technology\\

\section{{REPOSITORIES}}
\vspace{-10pt}
\hrulefill\\
{\bf GitHub} \hfill https://github.com/ddfamou

\end{resume}
\end{document}
